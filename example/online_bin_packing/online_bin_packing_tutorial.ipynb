{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ba1915fced4e72",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tutorial on online bin packing problem\n",
    "## Please open in Colab !!!\n",
    "Five steps:\n",
    "1. Implement a sampler\n",
    "2. Implement a evaluator\n",
    "3. Prepare a template heuristic\n",
    "4. Choose the LLM-EPS method and complete the config, and run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d02b8e9c3ba67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparation: download the project file from GitHub. And update system path."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download code from GitHub."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34937ecb66772bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f419cd674eb4fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf py-alevo\n",
    "!git clone https://github.com/RayZhhh/py-alevo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22453e8153e0934c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/py-alevo/')\n",
    "sys.path.append('/content/py-alevo/example/online_bin_packing')\n",
    "\n",
    "from typing import Any, List\n",
    "\n",
    "import alevo\n",
    "from alevo.tools.profiler import TensorboardProfiler\n",
    "from _obp_evaluate import evaluate\n",
    "from alevo.tools.llm.llm_api_https import HttpsApi\n",
    "a = HttpsApi(host='', key='', model='', timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47175708cc0a93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Implement a sampler\n",
    "The sampler defines the way to use local LLM or LLM API. You should create a new Sampler class by implementing `alevo.base.Sampler`.\n",
    "- You should implement \"draw_sample\" function, to let the package know how to get a LLM's response by given a prompt.\n",
    "- If you want more acceleration (such as batch inference, multi-threading sampling) you can also override \"draw_samples\" function.\n",
    "- The following example shows a fake sampler, which returns a random function in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999e45c9a568b08",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class FakeSampler(alevo.base.Sampler):\n",
    "    \"\"\"We select random functions from rand_function.pkl\n",
    "    This sampler can help you debug your method even if you don't have an LLM API / deployed local LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            with open('data/rand_function.pkl', 'rb') as f:\n",
    "                self._functions = pickle.load(f) \n",
    "        except:\n",
    "            with open('/content/py-alevo/example/online_bin_packing/data/rand_function.pkl', 'rb') as f:\n",
    "                self._functions = pickle.load(f)\n",
    "\n",
    "    def draw_samples(self, prompts: List[str | Any], *args, **kwargs) -> List[str]:\n",
    "        return super().draw_samples(prompts)\n",
    "\n",
    "    def draw_sample(self, prompt: str | Any, *args, **kwargs) -> str:\n",
    "        \"\"\"Generate an LLM response of the given prompt.\n",
    "        \"\"\"\n",
    "        return random.choice(self._functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91927f03a7bc468",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test: get a sample\n",
    "sampler = FakeSampler()\n",
    "sample = sampler.draw_sample('Fake sampler does not need a prompt.')\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051608732d45a2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following example shows a sampler that uses GPT-3.5-turbo API. If you want to use this sampler in this notebook, please complete following two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1012894e4440b4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api_endpoint: str = ''  # the ip of your API provider, no \"https://\", such as \"api.bltcy.top\".\n",
    "api_key: str = ''  # your API key which may start with \"sk-......\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a0da7632055aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import http.client\n",
    "import json\n",
    "\n",
    "\n",
    "class MySampler(alevo.base.Sampler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def draw_samples(self, prompts: List[str | Any], *args, **kwargs) -> List[str]:\n",
    "        return super().draw_samples(prompts)\n",
    "\n",
    "    def draw_sample(self, prompt: str | Any, *args, **kwargs) -> str:\n",
    "        while True:\n",
    "            try:\n",
    "                conn = http.client.HTTPSConnection(f'{api_endpoint}', timeout=30)\n",
    "                payload = json.dumps({\n",
    "                    'max_tokens': 512,\n",
    "                    'model': 'gpt-3.5-turbo',\n",
    "                    'messages': [{'role': 'user', 'content': prompt}]\n",
    "                })\n",
    "                headers = {\n",
    "                    'Authorization': f'Bearer {api_key}',\n",
    "                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
    "                    'Content-Type': 'application/json'\n",
    "                }\n",
    "                conn.request('POST', '/v1/chat/completions', payload, headers)\n",
    "                res = conn.getresponse()\n",
    "                data = res.read().decode('utf-8')\n",
    "                data = json.loads(data)\n",
    "                response = data['choices'][0]['message']['content']\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(2)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df23f5d648424f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test 'MySampler' if you have an API key\n",
    "if api_key != '' and api_endpoint != '':\n",
    "    sampler = MySampler()\n",
    "    response = sampler.draw_sample('Hello!')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27817cdec2cedfc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Implement an evaluator\n",
    "The evaluator defines how to evaluate the generated heuristic function. You should create a new Evaluator class by implementing `alevo.base.Evaluator`. You should override \"evaluate_program\" function to specify. Return None if the function is invalid.\n",
    "\n",
    "The `alevo.base.Evaluator` class provide acceleration and safe evaluation methods. You can use them by simply setting respective arguments. The commonly used two argument are:\n",
    "- `use_numba_accelerate`: If set to True, we will wrap the heuristic function with '@numba.jit(nopython=True)'. Please note that not all functions support numba.jit(), so use it appropriately.\n",
    "- `timeout_second`: Terminate the evaluation after timeout seconds. If set to `None`, the evaluator will wait until the evaluation finish.\n",
    "\n",
    "For more arguments, please refer to docstring in `algo.base.Evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d88a87535b6b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OBPEvaluator(alevo.base.Evaluator):\n",
    "    \"\"\"Evaluator for online bin packing problem.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            use_numba_accelerate=True,\n",
    "            timeout_seconds=10\n",
    "        )\n",
    "        try:\n",
    "            with open('data/weibull_train.pkl', 'rb') as f:\n",
    "                self._bin_packing_or_train = pickle.load(f)['weibull_5k_train']\n",
    "        except:\n",
    "            with open('/content/py-alevo/example/online_bin_packing/data/weibull_train.pkl', 'rb') as f:\n",
    "                self._bin_packing_or_train = pickle.load(f)['weibull_5k_train']\n",
    "\n",
    "    def evaluate_program(self, program_str: str, callable_func: callable) -> Any | None:\n",
    "        \"\"\"Evaluate a given function. You can use compiled function (function_callable),\n",
    "        as well as the original function strings for evaluation.\n",
    "        Args:\n",
    "            program_str: The function in string. You can ignore this argument when implementation.\n",
    "            callable_func: The callable Python function of your sampled heuristic function code. \n",
    "            You can call the program using 'program_callable(args..., kwargs...)'\n",
    "        Return:\n",
    "            Returns the fitness value. Return None if you think the result is invalid.\n",
    "        \"\"\"\n",
    "        # we call the _obp_evaluate.evaluate function to evaluate the callable code\n",
    "        return evaluate(self._bin_packing_or_train, callable_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator = OBPEvaluator()\n",
    "secure_evaluator = alevo.base.SecureEvaluator(evaluator=evaluator, debug_mode=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d7221c858241d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb26c049543591",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test the evaluator\n",
    "test_program = '''\n",
    "import numpy as np\n",
    "\n",
    "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns priority with which we want to add item to each bin.\n",
    "    Args:\n",
    "        item: Size of item to be added to the bin.\n",
    "        bins: Array of capacities for each bin.\n",
    "    Return:\n",
    "        Array of same size as bins with priority score of each bin.\n",
    "    \"\"\"\n",
    "    return bins - item\n",
    "'''\n",
    "\n",
    "res = secure_evaluator.evaluate_program(test_program)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a41b77f092768",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we test an invalid program\n",
    "test_program = '''\n",
    "import numpy as np\n",
    "\n",
    "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns priority with which we want to add item to each bin.\n",
    "    Args:\n",
    "        item: Size of item to be added to the bin.\n",
    "        bins: Array of capacities for each bin.\n",
    "    Return:\n",
    "        Array of same size as bins with priority score of each bin.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        pass\n",
    "'''\n",
    "\n",
    "res = secure_evaluator.evaluate_program(test_program)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2be731d3ebfab2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Implement a template program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94605bcc1f9e0e32",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template_program = '''\n",
    "import numpy as np\n",
    "\n",
    "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns priority with which we want to add item to each bin.\n",
    "    Args:\n",
    "        item: Size of item to be added to the bin.\n",
    "        bins: Array of capacities for each bin.\n",
    "    Return:\n",
    "        Array of same size as bins with priority score of each bin.\n",
    "    \"\"\"\n",
    "    penalty = np.arange(len(bins), 0, -1)\n",
    "    scores = bins / (bins - item) - penalty\n",
    "    max_capacity_bins = np.where(bins == bins.max())[0]\n",
    "    for idx in max_capacity_bins:\n",
    "        scores[idx] = -np.inf\n",
    "    return scores\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06b56e75ef7703",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Choose the LLM-EPS method and complete the config, and run\n",
    "Our package support multiprocess running. However, the Colab backend has limited CPU support, so we set num_evlauators to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Common args in Config:\n",
    "- `num_samplers`: number of threads used in sampling.\n",
    "- `num_evaluators`: number of processes used in evaluation (supports using multi-core CPUs)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2885bdfce4b89ee9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cd55e162f0b94",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_randsample():\n",
    "    from alevo.method.randsample import RandSample, Config\n",
    "    profiler = TensorboardProfiler(log_dir='')\n",
    "    config = Config(\n",
    "        num_samplers=4,\n",
    "        num_evaluators=2,\n",
    "    )\n",
    "    randsample = RandSample(template_program, sampler, evaluator, profiler, config=config, max_sample_nums=100)\n",
    "    randsample.run()\n",
    "\n",
    "\n",
    "def run_hillclimb():\n",
    "    from alevo.method.hillclimb import HillClimb, Config\n",
    "    profiler = TensorboardProfiler(log_dir='')\n",
    "    config = Config(\n",
    "        num_samplers=4,\n",
    "        num_evaluators=2,\n",
    "    )\n",
    "    hillclimb = HillClimb(template_program, sampler, evaluator, profiler, config=config, max_sample_nums=100)\n",
    "    hillclimb.run()\n",
    "\n",
    "\n",
    "def run_funsearch():\n",
    "    from alevo.method.funsearch import FunSearch, Config\n",
    "    profiler = TensorboardProfiler(log_dir='')\n",
    "    config = Config(\n",
    "        num_samplers=2,\n",
    "        num_evaluators=2,\n",
    "        samples_per_prompt=2\n",
    "    )\n",
    "    funsearch = FunSearch(template_program, sampler, evaluator, profiler, config=config, max_sample_nums=100)\n",
    "    funsearch.run()\n",
    "\n",
    "\n",
    "def run_eoh():\n",
    "    # Please note that you can simply pass the template program without function body, such as:\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # import numpy as np\n",
    "    # \n",
    "    # def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    #     \"\"\"Returns priority with which we want to add item to each bin.\n",
    "    #     Args:\n",
    "    #         item: Size of item to be added to the bin.\n",
    "    #         bins: Array of capacities for each bin.\n",
    "    #     Return:\n",
    "    #         Array of same size as bins with priority score of each bin.\n",
    "    #     \"\"\"\n",
    "    #     pass\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # This is because EoH can sample a valid code by sampling.\n",
    "    from alevo.method.eoh import EoH, Config\n",
    "    profiler = TensorboardProfiler(log_dir='')\n",
    "    config = Config(\n",
    "        num_samplers=2,\n",
    "        num_evaluators=2,\n",
    "        pop_size=20\n",
    "    )\n",
    "    # You can choose to add some task descriptions or not.\n",
    "    task_description = \"\"\"Please help me design a heuristic function for Online Bin Packing function. Given a item with its size S, the heuristic function finds the most suitable bin with remaining capacity C to pack it. We want to design a heuristic function that evaluate the priority of bins to which we want to assign the item to each bin.\"\"\"\n",
    "    eoh = EoH(\n",
    "        task_description=task_description,\n",
    "        template_program=template_program,\n",
    "        config=config,\n",
    "        max_generations=3,\n",
    "        sampler=sampler,\n",
    "        evaluator=evaluator,\n",
    "        profiler=profiler\n",
    "    )\n",
    "    eoh.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4da616889236c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It should be noted that the if __name__ == '__main__' is required.\n",
    "# Because the inner code uses multiprocess evaluation.\n",
    "if __name__ == '__main__':\n",
    "    # you can also try other LLM-EPS methods.\n",
    "    run_randsample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
